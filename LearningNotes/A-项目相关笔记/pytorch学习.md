## 笔记：线性模型

1.准备数据集
2.模型选择，模型设计（神经网络，决策树等）
3.训练
4.推理，测试

#### 训练
1.一般数据集分为训练集，开发集，测试集。竞赛中测试集无法看到输出结果（否则会导致作弊），训练集已知输入输出，开发集一般是从训练集里自己分出来的。
2.数据集和用户的实际数据可能有较大差距，尽量爬取各方数据，数据需要全面。
3.机器学习核心难题为过拟合，我们希望模型有泛化能力（即很好地识别不认识的数据）

#### 线性模型
1.一般来说，我们会找一个函数，作为最佳模型。
2.但是在机器学习中，常见的思路是先拿线性模型看一下是否有效，因为它可以作为最基本架构，如果效果不好再更换其它模型。
3.在机器学习中，如果数据不太好判断，我们会先随机地做一个权重预测，即猜一个权重。
4.需要看猜测权重得出的值和数据集中的真实值的区别，需要一个评估模型，得知找到的模型模拟的结果与实际数据的差别有多大，该差别叫做损失（loss）
5.目标是找一个合适的权重值，让平均的loss降到最低。
6.对整个样本分析，由平均平方误差（MSE）获得样本误差。
7.根据不同权重看MSE的值，用来调整权重。  

## 笔记：梯度下降算法  
一般来说，我们需要考虑很多权重，因此需要大量处理数据，如果没有一个优秀的算法，我们很难把最终的结果算出来，因为数据量太大。  
#### 分治法为什么行不通  
代表权重的曲线一般不是理想光滑的，可能出现大量不规律的波动，我们很容易找错点，误以为某个点内含的是理想权重，实际上我们如果处理所有数据，可能会发现更低的点。总而言之，分治法有风险，而且风险很大。  
当然，如果曲线相当光滑且是凸函数或者凹函数，貌似还是可以用分治法的。

#### 优化问题（optimization problem）
寻找使目标函数最小的权重组合的任务，称为优化问题。  
#### 梯度下降算法（gradient descent algorithm）  
**梯度**：用目标函数对权重求导
根据梯度，我们会对权重进行更新，使用一种公式。
![公式](image-4.png)  
公式中的阿尔法被称为学习率，学习率决定点移动的距离（因此应该设计得尽量小一点，步子放窄）。这就是梯度下降算法的核心公式。  



*我们每一次迭代的时候，都朝着下降最快的方向走，即使得cost最小。梯度下降不一定能得到最优的结果，但是能得到局部区域的最优结果*  
*比如如果函数呈波浪形，运气不好的话，我们就只能找到局部最优，而不是全局最优*  

但是，局部最优点在实际情况看其实很少，这也是深度学习大量运用梯度下降算法的原因。不过有一种叫鞍点的玩意不太友好，即函数在某一段是平的，根据梯度下降公式，我们无法更新权重了。  
以下是梯度下降公式推导过程和它的完全体。
![梯度下降公式完全体](image.png)  


以下是实际情况下常用的公式，通过随机选取某些噪声点来通过鞍点。
![随机梯度下降](image-1.png)  

**普通梯度下降算法训练的模型，最终性能低，但是所用训练时间少。随机梯度下降算法训练的模型，最终性能高，但是所用时间也更多。**

## 笔记：反向传播Back Propagation  
*面对复杂的神经网络，可能有几百个权重，要一个个写出解析式几乎是不可能的，因此我们需要一种算法，把复杂的神经网络看作是一个图，我们可以在图上传播梯度，最终根据链式法则，把梯度求出来。*  
由输入计算最终的LOSS，求输出f(x,w)=z中，损失loss关于输出z的偏导。最终通过反向的链式法则，求损失L关于输入x的偏导和损失L关于权重w的偏导。

## 笔记：用pytorch实现线性回归  
1. 准备数据集  
2. 设计模型  
3. 构造损失函数与优化器（这时我们需要使用pytorch的应用接口）
4. 写训练周期  
一般来说，输入的值是多维的，可以看作是一个矩阵，那么权重自然也是一个矩阵，其与输入值矩阵相乘所得的矩阵，要符合y（hat）的维度。  
同时我们可能会得到关于损失函数的矩阵，此时我们要对矩阵中的损失函数加和，总之，最后的损失函数一定是标量，否则我们无法使用backward进行反向传播。  
在pytorch里面做模型的模板如下：
    class LinearModel(torch.nn.Module):
    #从module模块中继承  
    def \_\_init\_\_(self):
        super(LinearModel, self).\_\_init__()
        self.linear = torch.nn.Linear(1, 1)
    def forward(self, x):
        y_pred = self.linear(x)
        return y_pred
    model = LinearModel()

criterion = torch.nn.MSELoss(size_average=False)
optimizer = torch.optim.SGD(model.parameters(), lr=0.01)  

for epoch in range(100)
    y_pred = model(x_data)
    loss = criterion(y_pred, y_data)
    print(epoch, loss)  
    optimizer.zero_grad()
    #梯度归零
    loss.backward()
    #反向传播
    optimizer.step()
    #优化梯度  

\#output weight and bias  
print("w = ", model.linear.weight.item())
print("b = ", model.linear.bias.item())  

\#最后测试
x_test = torch.Tensor(\[[4.0]])
y_test = model(x_test)
print("y_pred = ", y_test.data)
![全部代码](image-2.png)



## 逻辑斯蒂回归

使得输出的值在0到1之间，运用在分类中求出概率，一般会用到logistic function。

![逻辑斯蒂回归](image-20230926202745296.png)

这种函数在数学中一般称为饱和函数，x=0时导数最大，x趋向正无穷时导数越来越小，x趋向负无穷时导数同样越来越小，直到无限趋近于0。

![如图](屏幕截图 2023-09-26 203722.png)

这里是其他的sigmoid函数，共同的特点是都有极限，都是单调增函数与饱和函数。一般满足类似函数的都叫sigmoid函数，但是logistic函数是最出名的。



在sigmoid函数加持下，我们输出的是一个分布，这个时候损失函数的求法也要改

二分类的损失函数称为BCE，我们也可以对小批量数据的损失函数进行求和。比起先前的代码，我们只在定义forward函数时，使用了F.sigmoid(self.linear(x))，而不是直接写self.linear(x)。





## 处理多维特征的输入

![image-20230926232930274](image-20230926232930274.png)

当x为多维时，我们就要对之前学的逻辑斯蒂回归做一个小小的修改。

pytorch提供的sigmoid函数是按向量计算的函数，和numpy非常接近。转化为矩阵运算可以利用CPU和GPU的并行计算能力，提高运算效率。如果是多维输入的话，就要对源代码做一个修改。

![image-20230927093401697](image-20230927093401697.png)

如图，假如是八维的特征输入，就需要修改为

```python
self.Linear = torch.nn.Linear(8, 1) 
```

意思就是把八维通过线性变换映射到一维，当然也可以分布映射，比如8维到6维，6维到2维这样子变，本质上是矩阵的变化。我们通过引入激活函数（sigmoid函数），给线性变换增加非线性的操作，使我们能够拟合非线性的变换。维度在神经网络的不同层中不断变换，可以往低维变化，也可以往高维变化。



## 加载数据集

将全部数据集分割成多个便于检测模型性能，训练时嵌套循环，进行迭代。

epoch指的是所有数据进行了一次完整的正向传播和反向传播，batch-size指一次训练过程中使用的样本数量。

iteration，举个形象点的例子，数据共有10000个，一个batch-size为1000.那么iteration为10。



## 多分类问题

为了处理多分类问题，保证各种分类的概率总和为1，且概率大于等于0，在sigmoid层之外，我们还需要做一个层，称为softmax。

功能是将神经网络计算出的值转化为正值，并且让它们的和等于1，解决这两个问题。

没有一刻为看不懂代码的哥们哀悼，立刻赶往战场的是！！！softmax！！

![softmax计算公式](image-20230927202228712.png)

损失函数-交叉熵，用一个新方法求多分类的损失函数。

![损失函数](image-20230927203201338.png)

![这是使用pytorch求交叉熵得出的损失函数](image-20230927215115824.png)

y需要是longtensor，即为长整型的张量。



## 卷积神经网络（基础篇）

经过卷积，图像的通道、宽度和高度都有可能发生改变，然后我们会进行下采样（减少数据量）。

下采样时，图像的通道数不会改变，但是宽度和高度会变，一般用于使计算更加高效。

经过不断的重复卷积和下采样，使得所有单个数据能按一定顺序排成一列，获得线性的、向量的输入，最后再利用全连接层，映射到一个多维的输出。最后用交叉熵损失，利用softmax，获得概率分布。最终解决分类问题。

一般是从整个图像中取一个图像块做卷积操作。

如果是多个通道的卷积，每一个通道都需要配置一个相应的卷积核。

![多通道卷积](image-20230930220240270.png)
然后再把三个通道的卷积结果进行一一对应的相加，这就是卷积的运算。

![卷积得到多个通道](image-20230930221101239.png)

这里是由卷积得到多个通道的方法，就是选择更多的卷积核，获得比原来更多的通道。

注意：

1. 卷积核的通道数需要和输入图像的通道数相等。
2. 卷积核的个数决定输出的通道数。



![写卷积的代码示例](image-20230930222126608.png)



**padding**：可以使输入外面多1到n圈，影响输出的W*H。
以下是padding的代码运用

![使用padding的示例代码](image-20230930222852738.png)


**stride**：步长，即卷积核的中心每次移动的格数
![代码示例](image-20230930223235144.png)



下采样中，我们运用最多的方法是maxpooling。下采样不影响通道数量，他会取每一个分部中的最大值，并将每个最大值组成一个新的层


![把模型放到显卡里计算](image-20230930224416527.png)