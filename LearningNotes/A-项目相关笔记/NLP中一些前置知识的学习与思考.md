# 前置知识的学习与思考

### 只简单查阅了资料，了解了大概



1. **multi-label classification&multi-class classification**
   - multi-label classification为多标签分类，一个样本可以被分配多个标签，每个样本可以属于一个或多个类别。
   - multi-class classification为多类分类，一个样本只能被分配一个标签，每个样本只属于一个类别
   - 一般当各个类之间是完全互斥的时候，用多类分类的方法，将每个样本准确分类到单个类别中，一个经典例子就是MNIST手写数据集的识别；而如果需要为每个样本确定所属的多个标签组合，则需要多标签分类输出二进制向量，每一个元素表示一个标签存在与否。

 

2. **NLP任务的一般流程有哪些？分词&去除停用词是干什么的？**

- *NLP一般流程*

  1. 语料获取，拿到文本数据
  2. 预处理，去掉一些无关紧要的文本，打一打标签，使计算机更好理解文本
     - 分词
     - 去除停用词
     - 文本规范化
     - 特征提取
  3. 使处理后的文本能用于任务计算，即文本向量化
  4. 构建模型并训练
  

- *分词&去除停用词*
  
  1. 分词
  
     将连续文本切割成离散单词，或者说划分出有意义的语言单元，方便计算机更好地理解和处理文本。
  
  2. 去除停用词
  
     没必要存在的词全部去除，去除也不影响语义。提高训练效率
  
     
  
3. **词向量/word2vec是什么？语料库（corpus）是什么**？

- 词向量

  一种NLP技术，将词转化为稠密向量，相似的词，词向量相近。

- word2vec

  常见的词向量模型，使具有相同含义的单词在向量空间上更接近。

- corpus

  用于训练和研究的大量文本数据集合。对训练出高质量词向量很重要。

  

4. **attention机制**

允许模型根据输入数据不同部分的重要性来进行加权求和。

*使关注重点，重点关注重要的部分。*



5. **Recurrent Network**

递归网络，允许信息在网络内部传递和持久化，最常见的结构是循环神经网络。可以捕捉到数据的时间顺序，越晚的输入影响越大。



6. **LSTM&Bi-LSTM**

LSTM，递归神经网络RNN的变体，解决RNN的短时记忆短板。在RNN基础上增加三个门控，主打的就是一个抓重点。

Bi-LSTM是LSTM的扩展，结合了正向和反向的 LSTM，以利用双向上下文信息。